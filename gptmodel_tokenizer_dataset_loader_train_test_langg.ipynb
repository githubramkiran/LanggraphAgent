{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 13992433,
          "sourceType": "datasetVersion",
          "datasetId": 8917591
        },
        {
          "sourceId": 14473575,
          "sourceType": "datasetVersion",
          "datasetId": 9244536
        },
        {
          "sourceId": 14505757,
          "sourceType": "datasetVersion",
          "datasetId": 9264829
        }
      ],
      "dockerImageVersionId": 31236,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/githubramkiran/LanggraphAgent/blob/main/gptmodel_tokenizer_dataset_loader_train_test_langg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gpt model with bpe tokenizer,dataset loader,training loop,testing loop and langgraph integration code\n"
      ],
      "metadata": {
        "id": "qWHFEdKtHYgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langgraph"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-15T09:50:53.379335Z",
          "iopub.execute_input": "2026-01-15T09:50:53.379716Z",
          "iopub.status.idle": "2026-01-15T09:51:04.039061Z",
          "shell.execute_reply.started": "2026-01-15T09:50:53.379693Z",
          "shell.execute_reply": "2026-01-15T09:51:04.038363Z"
        },
        "id": "BN1uLzXlHYgS",
        "outputId": "fd3f4469-94a1-4e1f-d9d9-4ec43a6423ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\nCollecting langgraph\n  Downloading langgraph-1.0.6-py3-none-any.whl.metadata (7.4 kB)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\nRequirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.37)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.5)\nCollecting SQLAlchemy<3,>=1.4 (from langchain)\n  Downloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\nCollecting langgraph-checkpoint<5.0.0,>=2.1.0 (from langgraph)\n  Downloading langgraph_checkpoint-4.0.0-py3-none-any.whl.metadata (4.9 kB)\nCollecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph)\n  Downloading langgraph_prebuilt-1.0.6-py3-none-any.whl.metadata (5.2 kB)\nCollecting langgraph-sdk<0.4.0,>=0.3.0 (from langgraph)\n  Downloading langgraph_sdk-0.3.3-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\nRequirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\nRequirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\nCollecting ormsgpack>=1.12.0 (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph)\n  Downloading ormsgpack-1.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\nINFO: pip is looking at multiple versions of langgraph-prebuilt to determine which version is compatible with other requirements. This could take a while.\nCollecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph)\n  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n  Downloading langgraph_prebuilt-1.0.4-py3-none-any.whl.metadata (5.2 kB)\n  Downloading langgraph_prebuilt-1.0.3-py3-none-any.whl.metadata (5.2 kB)\n  Downloading langgraph_prebuilt-1.0.2-py3-none-any.whl.metadata (5.0 kB)\nCollecting langgraph-checkpoint<5.0.0,>=2.1.0 (from langgraph)\n  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\nINFO: pip is still looking at multiple versions of langgraph-prebuilt to determine which version is compatible with other requirements. This could take a while.\n  Downloading langgraph_checkpoint-3.0.0-py3-none-any.whl.metadata (4.2 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading langgraph_checkpoint-2.1.2-py3-none-any.whl.metadata (4.2 kB)\n  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n  Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\nCollecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n  Downloading langchain_text_splitters-0.3.10-py3-none-any.whl.metadata (1.9 kB)\nCollecting pip>=25.2 (from langchain-text-splitters<1.0.0,>=0.3.9->langchain)\n  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\nCollecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\nCollecting langgraph\n  Downloading langgraph-1.0.5-py3-none-any.whl.metadata (7.4 kB)\n  Downloading langgraph-1.0.4-py3-none-any.whl.metadata (7.8 kB)\nCollecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n  Downloading langgraph_sdk-0.2.15-py3-none-any.whl.metadata (1.6 kB)\nCollecting langgraph\n  Downloading langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n  Downloading langgraph-1.0.2-py3-none-any.whl.metadata (7.4 kB)\n  Downloading langgraph-1.0.1-py3-none-any.whl.metadata (7.4 kB)\nCollecting langgraph-prebuilt<1.1.0,>=1.0.0 (from langgraph)\n  Downloading langgraph_prebuilt-1.0.1-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\nRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.11.12)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.12.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\nDownloading langgraph-1.0.1-py3-none-any.whl (155 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langgraph_prebuilt-1.0.1-py3-none-any.whl (28 kB)\nDownloading langgraph_sdk-0.2.15-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading ormsgpack-1.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: SQLAlchemy, ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n  Attempting uninstall: SQLAlchemy\n    Found existing installation: SQLAlchemy 1.2.19\n    Uninstalling SQLAlchemy-1.2.19:\n      Successfully uninstalled SQLAlchemy-1.2.19\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nxmanager 0.7.1 requires sqlalchemy==1.2.19, but you have sqlalchemy 2.0.45 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed SQLAlchemy-2.0.45 langgraph-1.0.1 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.1 langgraph-sdk-0.2.15 ormsgpack-1.12.1\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Byte Pair Encoding (BPE) Tokenizer\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "# Initialize BPE model with byte-level pre-tokenization\n",
        "#tokenizer = Tokenizer(models.BPE(byte_fallback=True))\n",
        "#tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
        "\n",
        "# Initialize BPE tokenizer\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Train on a text file\n",
        "trainer = trainers.BpeTrainer(vocab_size=50000, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "tokenizer.train([\"/kaggle/input/langchain/langchain.txt\"], trainer)\n",
        "tokenizer.save(\"gpt_tokenizer.json\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-15T09:51:04.040646Z",
          "iopub.execute_input": "2026-01-15T09:51:04.040895Z",
          "iopub.status.idle": "2026-01-15T09:51:04.196911Z",
          "shell.execute_reply.started": "2026-01-15T09:51:04.040871Z",
          "shell.execute_reply": "2026-01-15T09:51:04.196320Z"
        },
        "id": "4pQ9f8c6HYga",
        "outputId": "dbdbbdce-a628-489a-fe0e-2ffa855459d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, text_path, tokenizer, block_size):\n",
        "        with open(text_path, 'r') as f:\n",
        "            text = f.read()\n",
        "        self.tokens = tokenizer.encode(text).ids\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # x is the sequence, y is x shifted by 1 (next-token prediction)\n",
        "        chunk = self.tokens[idx : idx + self.block_size + 1]\n",
        "        return torch.tensor(chunk[:-1]), torch.tensor(chunk[1:])\n",
        "\n",
        "# Initialize Loader\n",
        "dataset = GPTDataset(\"/kaggle/input/langchain/langchain.txt\", tokenizer, block_size=128)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-15T09:51:04.197769Z",
          "iopub.execute_input": "2026-01-15T09:51:04.198037Z",
          "iopub.status.idle": "2026-01-15T09:51:09.886442Z",
          "shell.execute_reply.started": "2026-01-15T09:51:04.198009Z",
          "shell.execute_reply": "2026-01-15T09:51:09.885609Z"
        },
        "id": "PRY_dRthHYgi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\" A single GPT decoder block: communication followed by computation \"\"\"\n",
        "    def __init__(self, n_embd, n_head, block_size, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Causal multi-head self-attention\n",
        "        self.sa = nn.MultiheadAttention(\n",
        "            embed_dim=n_embd,\n",
        "            num_heads=n_head,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        # Feed-forward network (computation)\n",
        "        self.ffwd = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        # Layer normalization\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "        # Causal mask to ensure next-token prediction behavior\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)) == 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, seq_len, n_embd)\n",
        "        sz = x.size(1)\n",
        "        # 1. Multi-head Attention with Causal Masking\n",
        "        # Residual connection (x + sa) applied after LayerNorm (Pre-norm)\n",
        "        attn_mask = self.mask[:sz, :sz]\n",
        "        attn_output, _ = self.sa(self.ln1(x), self.ln1(x), self.ln1(x), attn_mask=attn_mask)\n",
        "        x = x + attn_output\n",
        "\n",
        "        # 2. Feed-Forward Network\n",
        "        # Residual connection (x + ffwd) applied after LayerNorm\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-15T09:51:09.887954Z",
          "iopub.execute_input": "2026-01-15T09:51:09.888325Z",
          "iopub.status.idle": "2026-01-15T09:51:09.894957Z",
          "shell.execute_reply.started": "2026-01-15T09:51:09.888303Z",
          "shell.execute_reply": "2026-01-15T09:51:09.894245Z"
        },
        "id": "lNj-7EJ1HYgl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class CustomGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # Token and Learned Positional Embeddings\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        # Stack of Transformer Decoder Blocks\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(n_embd, n_head, block_size, dropout) for _ in range(n_layer)\n",
        "        ])\n",
        "\n",
        "        # Final LayerNorm and Linear Head to vocabulary\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # Combine token (what) and position (where) embeddings\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
        "\n",
        "        # Pass through the stack of blocks\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Shifted cross-entropy for next-token prediction\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = nn.functional.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # Autoregressive generation loop\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop index to the block size\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            # Focus on the last time step\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = nn.functional.softmax(logits, dim=-1)\n",
        "            # Sample next token\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "model=CustomGPT(vocab_size=tokenizer.get_vocab_size(), n_embd=128, n_head=4, n_layer=2, block_size=64, dropout=0.1)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "device=\"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using NVIDIA GPU\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using Apple MPS (Metal Performance Shaders)\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-15T09:51:09.895870Z",
          "iopub.execute_input": "2026-01-15T09:51:09.896235Z",
          "iopub.status.idle": "2026-01-15T09:51:13.390209Z",
          "shell.execute_reply.started": "2026-01-15T09:51:09.896204Z",
          "shell.execute_reply": "2026-01-15T09:51:13.389540Z"
        },
        "id": "W_S2Og_7HYgn",
        "outputId": "8aa8c6d4-f8af-4f9d-87ed-966a5f2b8838"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using NVIDIA GPU\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_gpt(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits, loss = model(x, y) # Model returns loss via internal CrossEntropy\n",
        "        loss.backward()\n",
        "        # Update model weights\n",
        "        optimizer.step()\n",
        "\n",
        "print('model.summary()',model)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('total_params',total_params)\n",
        "print('trainable_params',trainable_params)\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.shape)\n",
        "# Example output: fc.weight torch.Size([2, 96])\n",
        "torch.save(model.state_dict(), 'model_weights_v1.pth')\n",
        "\n",
        "def test_gpt(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            _, loss = model(x, y)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-15T09:51:13.391210Z",
          "iopub.execute_input": "2026-01-15T09:51:13.391881Z",
          "iopub.status.idle": "2026-01-15T09:51:13.412275Z",
          "shell.execute_reply.started": "2026-01-15T09:51:13.391856Z",
          "shell.execute_reply": "2026-01-15T09:51:13.411566Z"
        },
        "id": "wEV1g02ZHYgv",
        "outputId": "c4eeb5fd-753f-408e-e9ff-b0093c1fe4ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "model.summary() CustomGPT(\n  (token_embedding_table): Embedding(461, 128)\n  (position_embedding_table): Embedding(64, 128)\n  (blocks): Sequential(\n    (0): TransformerBlock(\n      (sa): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n      )\n      (ffwd): Sequential(\n        (0): Linear(in_features=128, out_features=512, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=512, out_features=128, bias=True)\n        (3): Dropout(p=0.1, inplace=False)\n      )\n      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    )\n    (1): TransformerBlock(\n      (sa): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n      )\n      (ffwd): Sequential(\n        (0): Linear(in_features=128, out_features=512, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=512, out_features=128, bias=True)\n        (3): Dropout(p=0.1, inplace=False)\n      )\n      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n  (lm_head): Linear(in_features=128, out_features=461, bias=True)\n)\ntotal_params 523469\ntrainable_params 523469\ntoken_embedding_table.weight torch.Size([461, 128])\nposition_embedding_table.weight torch.Size([64, 128])\nblocks.0.sa.in_proj_weight torch.Size([384, 128])\nblocks.0.sa.in_proj_bias torch.Size([384])\nblocks.0.sa.out_proj.weight torch.Size([128, 128])\nblocks.0.sa.out_proj.bias torch.Size([128])\nblocks.0.ffwd.0.weight torch.Size([512, 128])\nblocks.0.ffwd.0.bias torch.Size([512])\nblocks.0.ffwd.2.weight torch.Size([128, 512])\nblocks.0.ffwd.2.bias torch.Size([128])\nblocks.0.ln1.weight torch.Size([128])\nblocks.0.ln1.bias torch.Size([128])\nblocks.0.ln2.weight torch.Size([128])\nblocks.0.ln2.bias torch.Size([128])\nblocks.1.sa.in_proj_weight torch.Size([384, 128])\nblocks.1.sa.in_proj_bias torch.Size([384])\nblocks.1.sa.out_proj.weight torch.Size([128, 128])\nblocks.1.sa.out_proj.bias torch.Size([128])\nblocks.1.ffwd.0.weight torch.Size([512, 128])\nblocks.1.ffwd.0.bias torch.Size([512])\nblocks.1.ffwd.2.weight torch.Size([128, 512])\nblocks.1.ffwd.2.bias torch.Size([128])\nblocks.1.ln1.weight torch.Size([128])\nblocks.1.ln1.bias torch.Size([128])\nblocks.1.ln2.weight torch.Size([128])\nblocks.1.ln2.bias torch.Size([128])\nln_f.weight torch.Size([128])\nln_f.bias torch.Size([128])\nlm_head.weight torch.Size([461, 128])\nlm_head.bias torch.Size([461])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# Define persistent state for the agent\n",
        "class GPTState(TypedDict):\n",
        "    history: list[str]  # Stores conversation history\n",
        "    input_text: str     # Current user query\n",
        "    generated_text:str\n",
        "\n",
        "def gpt_inference_node(state: GPTState):\n",
        "    # Prepare input for our PyTorch model\n",
        "    prompt = state[\"input_text\"]\n",
        "    #prompt = \" \".join(state[\"history\"] + [state[\"next_input\"]])\n",
        "    input_ids = torch.tensor([tokenizer.encode(prompt).ids])\n",
        "\n",
        "    # Generate response\n",
        "    generated_ids = model.generate(input_ids, max_new_tokens=20)\n",
        "    response = tokenizer.decode(generated_ids[0].tolist())\n",
        "    #print(response)\n",
        "    # Update state: append response to history\n",
        "    #return {\"history\": state[\"history\"] + [state[\"next_input\"], response]}\n",
        "    #response = tokenizer.decode(response)\n",
        "    return {\"generated_text\":response}\n",
        "\n",
        "# Assemble the Graph\n",
        "builder = StateGraph(GPTState)\n",
        "builder.add_node(\"generate\", gpt_inference_node)\n",
        "builder.add_edge(START, \"generate\")\n",
        "builder.add_edge(\"generate\", END)\n",
        "\n",
        "# Compile into a runnable application\n",
        "app = builder.compile()\n",
        "result = app.invoke({\"input_text\": \"What is langchain?\"})\n",
        "print(result)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-15T09:51:13.413066Z",
          "iopub.execute_input": "2026-01-15T09:51:13.413328Z",
          "iopub.status.idle": "2026-01-15T09:51:15.435387Z",
          "shell.execute_reply.started": "2026-01-15T09:51:13.413296Z",
          "shell.execute_reply": "2026-01-15T09:51:15.434697Z"
        },
        "id": "dQq6nqRsHYgy",
        "outputId": "335e0b14-44b0-46e9-be53-f443956d3a2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "{'input_text': 'What is langchain?', 'generated_text': 'W h at is langchain \": ast mes cor \": system_prompt heav lines are way \\u200b runtime ropic nec get wor e ent om \"\"'}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    input_text: str\n",
        "    generated_text: str\n",
        "\n",
        "# Define node that calls the custom PyTorch model\n",
        "def gpt_node(state: AgentState):\n",
        "    inputs = tokenizer.encode(state[\"input_text\"]).ids\n",
        "    # Generate tokens using your model's inference method\n",
        "    output_tokens = model.generate(torch.tensor([inputs]), max_new_tokens=50)\n",
        "    response = tokenizer.decode(output_tokens[0].tolist())\n",
        "    return {\"generated_text\": response}\n",
        "\n",
        "# Construct the graph\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"llm\", gpt_node)\n",
        "workflow.add_edge(START, \"llm\")\n",
        "workflow.add_edge(\"llm\", END)\n",
        "\n",
        "app = workflow.compile()\n",
        "result = app.invoke({\"input_text\": \"What is Transformers?\"})\n",
        "print(result[\"generated_text\"])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-15T09:51:15.436191Z",
          "iopub.execute_input": "2026-01-15T09:51:15.436415Z",
          "iopub.status.idle": "2026-01-15T09:51:15.589083Z",
          "shell.execute_reply.started": "2026-01-15T09:51:15.436395Z",
          "shell.execute_reply": "2026-01-15T09:51:15.588353Z"
        },
        "id": "1SLhWYRZHYg8",
        "outputId": "29e83718-9bee-4cd1-9b2c-ffef71e3786e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "W h at is r an s for m er s eed sist ve we ad aph omous fro } van 5 n _pro city by mp - can inst tion of the 20250929 have customization AI !\" way ap as carefully x qu omous re ecution re ec { hu ework runtim amless _ stom C workflow want .\"\"\" al\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}
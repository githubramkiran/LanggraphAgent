{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "langraph api endpoint",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/githubramkiran/LanggraphAgent/blob/main/langraph_api_endpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exposing a LangGraph agent as an API is slightly different from a standard LangChain agent because LangGraph is stateful. You need to handle thread IDs (conversation IDs) to ensure the graph remembers previous interactions."
      ],
      "metadata": {
        "id": "WQPHfGQFZFSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langgraph langchain_core langchain_community\n",
        "!pip install requests httpx fastapi uvicorn"
      ],
      "metadata": {
        "trusted": true,
        "id": "-gkm3ELiZFSn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "import os\n",
        "import getpass\n",
        "from langchain.chat_models import init_chat_model\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
        "llm = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n",
        "\n",
        "# 1. Define Graph & Memory (Same as above)\n",
        "#llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "checkpointer = MemorySaver()  # Use PostgresSaver for production\n",
        "\n",
        "\n",
        "def call_model(state: MessagesState):\n",
        "    return {\"messages\": llm.invoke(state[\"messages\"])}\n",
        "\n",
        "\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"chatbot\", call_model)\n",
        "builder.add_edge(START, \"chatbot\")\n",
        "graph = builder.compile(checkpointer=checkpointer)\n",
        "\n",
        "\n",
        "# 2. Define Request Schema\n",
        "class ChatRequest(BaseModel):\n",
        "    message: str\n",
        "    thread_id: str = \"default_thread\"\n",
        "\n",
        "\n",
        "# 3. Create Endpoint\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat_endpoint(request: ChatRequest):\n",
        "    try:\n",
        "        # Prepare input state\n",
        "        input_state = {\"messages\": [HumanMessage(content=request.message)]}\n",
        "\n",
        "        # Prepare config for state persistence\n",
        "        config = {\"configurable\": {\"thread_id\": request.thread_id}}\n",
        "\n",
        "        # Invoke Graph\n",
        "        # Use ainvoke for non-blocking async execution\n",
        "        result = await graph.ainvoke(input_state, config=config)\n",
        "\n",
        "        # Extract the last message content\n",
        "        last_message = result[\"messages\"][-1].content\n",
        "        return {\"response\": last_message, \"thread_id\": request.thread_id}\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "\n",
        "    uvicorn.run(app, host=\"localhost\", port=8000)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "EJfi3kLiZFS3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "curl -X POST http://localhost:8000/graph/invoke \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\n",
        "    \"input\": {\"messages\": [{\"role\": \"user\", \"content\": \"Hi, my name is Alice\"}]},\n",
        "    \"config\": {\"configurable\": {\"thread_id\": \"session_1\"}}\n",
        "  }'"
      ],
      "metadata": {
        "trusted": true,
        "id": "oclvnPXJZFS9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "http://0.0.0.0:8000/chat/\n",
        "body:\n",
        "{\n",
        "    \"message\": \"tell about rajahmundry\",\n",
        "    \"thread_id\": \"123\"\n",
        "}\n",
        "Content-Type:application/json"
      ],
      "metadata": {
        "id": "tx1Tu26BZFTD"
      }
    }
  ]
}
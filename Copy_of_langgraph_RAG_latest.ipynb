{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/githubramkiran/LanggraphAgent/blob/main/Copy_of_langgraph_RAG_latest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \"langchain-chroma>=0.1.2\" langchain langchain-openai langgraph\n",
        "!pip install -qU  langchain-google-genai langchain-community langchain-core"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-07T06:29:30.264795Z",
          "iopub.execute_input": "2025-12-07T06:29:30.265077Z",
          "iopub.status.idle": "2025-12-07T06:30:13.713886Z",
          "shell.execute_reply.started": "2025-12-07T06:29:30.265055Z",
          "shell.execute_reply": "2025-12-07T06:30:13.712824Z"
        },
        "id": "R2jWTjbsteRU",
        "outputId": "70a6757e-0c95-4081-915a-795d3cef8b5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "google-adk 1.20.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.20.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.20.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.20.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os,getpass\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your openai API key: \")\n",
        "#os.environ[\"LANGSMITH_TRACING\"] = \"true\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-07T06:30:13.716179Z",
          "iopub.execute_input": "2025-12-07T06:30:13.716479Z",
          "iopub.status.idle": "2025-12-07T06:30:45.875444Z",
          "shell.execute_reply.started": "2025-12-07T06:30:13.716451Z",
          "shell.execute_reply": "2025-12-07T06:30:45.874532Z"
        },
        "id": "NQjI0sm5teRn",
        "outputId": "7734e293-80d5-4fee-9a90-2a586a179d90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your openai API key: ··········\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import FakeEmbeddings\n",
        "embeddings = FakeEmbeddings(size=50)\n",
        "vector = embeddings.embed_query(\"hello, world!\")\n",
        "vector[:5]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-07T06:30:45.876325Z",
          "iopub.execute_input": "2025-12-07T06:30:45.876642Z",
          "iopub.status.idle": "2025-12-07T06:30:47.535101Z",
          "shell.execute_reply.started": "2025-12-07T06:30:45.876618Z",
          "shell.execute_reply": "2025-12-07T06:30:47.534106Z"
        },
        "id": "w9nVHbWbteRq",
        "outputId": "64901a1c-5860-4f2f-efc8-2b77ae28dd70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[np.float64(0.7614938460318729),\n",
              " np.float64(0.5901773488152734),\n",
              " np.float64(-0.5226592810013052),\n",
              " np.float64(-1.2221518111687573),\n",
              " np.float64(-0.5251848524518787)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"example_collection\",\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=\"./chroma_langchain_db\",\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-07T06:30:47.536355Z",
          "iopub.execute_input": "2025-12-07T06:30:47.536891Z",
          "iopub.status.idle": "2025-12-07T06:30:50.146707Z",
          "shell.execute_reply.started": "2025-12-07T06:30:47.536866Z",
          "shell.execute_reply": "2025-12-07T06:30:50.145802Z"
        },
        "id": "xkgHPnIUteRt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        "    id=1,\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        "    id=2,\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        "    id=3,\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "   ]\n",
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-07T06:30:50.148947Z",
          "iopub.execute_input": "2025-12-07T06:30:50.149344Z",
          "iopub.status.idle": "2025-12-07T06:30:50.210161Z",
          "shell.execute_reply.started": "2025-12-07T06:30:50.149323Z",
          "shell.execute_reply": "2025-12-07T06:30:50.209122Z"
        },
        "id": "4e_AOy53teRw",
        "outputId": "bf8e216f-98fe-4c87-c463-866e61c803dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['22a260b2-ad2a-490d-b748-231bb527905e',\n",
              " 'ac01bf12-511c-403d-aa53-d4b9378849bc',\n",
              " 'ffd443ad-e617-46b3-9c62-b5dce0c63131']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"mmr\", search_kwargs={\"k\": 1, \"fetch_k\": 5}\n",
        ")\n",
        "retriever.invoke(\"langchain\", filter={\"source\": \"news\"})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-07T06:30:50.211058Z",
          "iopub.execute_input": "2025-12-07T06:30:50.211366Z",
          "iopub.status.idle": "2025-12-07T06:30:50.24455Z",
          "shell.execute_reply.started": "2025-12-07T06:30:50.211339Z",
          "shell.execute_reply": "2025-12-07T06:30:50.243776Z"
        },
        "id": "-ngreF-yteRz",
        "outputId": "79dc8d4e-7321-4c32-cadd-6d2118472a58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='ac01bf12-511c-403d-aa53-d4b9378849bc', metadata={'source': 'news'}, page_content='The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.messages import AnyMessage\n",
        "from typing_extensions import TypedDict, Annotated\n",
        "import operator\n",
        "\n",
        "\n",
        "class MessagesState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], operator.add]\n",
        "    llm_calls: int\n",
        "    query:str\n",
        "    context:str\n",
        "\n",
        "# Step 3: Define model node\n",
        "from langchain.messages import SystemMessage\n",
        "\n",
        "#from langgraph.nodes import LLMNode\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain.tools import tool\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "#@tool\n",
        "def retriever_node(state: dict) -> str:\n",
        " response=retriever.invoke(query)\n",
        " print('retriver node response:',response)\n",
        " context=response[0]\n",
        " return {\"context\":context}\n",
        "# 2. Define nodes\n",
        "\n",
        "#tools = [retriever_node]\n",
        "#tool_node = ToolNode(tools)\n",
        "from langchain.messages import HumanMessage\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "def llm_call(state: dict):\n",
        "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
        "    print('llm call with state',query)\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            llm.invoke(\n",
        "                [HumanMessage(content=query),\n",
        "                    SystemMessage(\n",
        "                        content=\"You are a helpful assistant. Use the following context in your response:{context}\"\n",
        "                    )\n",
        "                ]\n",
        "                + state[\"messages\"]\n",
        "            )\n",
        "        ],\n",
        "        \"llm_calls\": state.get('llm_calls', 0) + 1\n",
        "    }\n",
        "# 3. Build graph\n",
        "#graph = Graph()\n",
        "agent_builder  = StateGraph(MessagesState)\n",
        "agent_builder .add_node(\"llm_call\", llm_call)\n",
        "agent_builder .add_node(\"tool_node\", retriever_node)\n",
        "\n",
        "# Connect retriever → generator\n",
        "\n",
        "# Add edges to connect nodes\n",
        "agent_builder .add_edge(START, \"tool_node\")#retriver\n",
        "agent_builder .add_edge(\"tool_node\", \"llm_call\")#generator\n",
        "agent_builder .add_edge(\"llm_call\",END)\n",
        "agent = agent_builder.compile()\n",
        "# 4. Run workflow\n",
        "query = \"What is langchain?\"\n",
        "from langchain.messages import HumanMessage\n",
        "messages = [HumanMessage(content=query)]\n",
        "messages = agent.invoke({\"messages\": messages})\n",
        "for m in messages[\"messages\"]:\n",
        "    m.pretty_print()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-07T07:48:32.764775Z",
          "iopub.execute_input": "2025-12-07T07:48:32.765112Z",
          "iopub.status.idle": "2025-12-07T07:48:35.270229Z",
          "shell.execute_reply.started": "2025-12-07T07:48:32.765079Z",
          "shell.execute_reply": "2025-12-07T07:48:35.269413Z"
        },
        "id": "6X1H_AnateR1",
        "outputId": "12439f21-2243-42d8-c4ef-0936482aaa7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "retriver node response: [Document(id='ac01bf12-511c-403d-aa53-d4b9378849bc', metadata={'source': 'news'}, page_content='The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.')]\n",
            "llm call with state What is langchain?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is langchain?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "LangChain is a framework designed to facilitate the development of applications that utilize large language models (LLMs). It provides a set of tools and components that help developers build applications that can interact with LLMs in a more structured and efficient way. LangChain supports various functionalities, including prompt management, memory management, and integration with external data sources, enabling developers to create complex workflows and applications that leverage the capabilities of language models. The framework is particularly useful for tasks such as chatbots, question-answering systems, and other natural language processing applications.\n"
          ]
        }
      ],
      "execution_count": null
    }
  ]
}